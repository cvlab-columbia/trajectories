# @package _global_
defaults:
  - override /dataset: finegym
  - override /model/optim_params: optim_deit
  - override /model/loss_params: loss_defaults
dataset:
  dataloader_params:
    batch_size: 200  # Initially option 2 (incorrect one) was size 100
    num_workers: 8
    persistent_workers: True
  dataset_params:
    max_steps: 10
    min_steps_past: 3
    min_steps_future: 3
checkpoint:
  save_top_k: 3
trainer:
  gpus: 4
  plugins: null
  max_epochs: 10000
model:
  name: TrajectoryModel
  time_encoder_strategy: 'fourier'
  time_decoder_strategy: 'fourier'
  latent_distribution: 'gaussian'
  point_trajectory: False
  num_sample_points: 3
  distance_type: 'optimal-transport'
  hidden_size: 512
  feature_size: 512
  option_reencode: 2
  num_classifier_layers: 4
  encoder_params:
    name: TransformerEncoder
    num_layers: 2
    nhead: 2
    in_size: 50  # Size of the points (25*2)
    return_cls: True
    dropout: 0
  decoder_params:
    name: ResnetFC
    in_size: 1
    out_size: 50  # Size of the points (25*2)
    n_blocks: 4
  loss_params:
    dict_losses:
      trajectory_loss:
        λ: 1
        params:
          loss_type: triplet
          margin: 1.
      reconstruction_loss:
        λ: 1
        params:
          distance_fn_name: 'euclidean_l1_keypoints'
          loss_type: 'regression'
  generate_extrapolation: True
  reconstruct_intersection: True
  reencode: True
  use_all: True
  optim_params:
    base_lr: 0.0001
    warmup_epochs: 1
    max_epoch: 500  # This is the number of training steps, not epochs
    warmup_start_lr: 0.00001
    grad_clip_val: 0.01
    grad_clip_strategy: 'norm'
setting: train
wandb:
  name: finegym_gaussian_ot_option2_l1_11positive
resume:
  id: smfecdq2  #3ieabpnc
  load_all: True
  epoch: last
  check_config: False
